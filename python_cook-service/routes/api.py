# routes/api.py
from fastapi import APIRouter, HTTPException
from models.models import SearchRequest, KeywordSearchRequest
from data.db import collection, embed_model
from unidecode import unidecode
from typing import List
import math
from data.indexing import sync_recipes_to_chroma

router = APIRouter()

@router.post("/search")
def search(req: SearchRequest):
    """
    TÃ¬m kiáº¿m theo danh sÃ¡ch nguyÃªn liá»‡u vá»›i scoring thÃ´ng minh:
    - KHÃ”NG dÃ¹ng vector search (khÃ´ng cáº§n semantic cho ingredient matching)
    - Filtering chÃ­nh xÃ¡c: Chá»‰ tráº£ vá» mÃ³n cÃ³ Tá»ˆ Lá»† MATCH â‰¥ 60%
    - Æ¯u tiÃªn mÃ³n Ã­t nguyÃªn liá»‡u (3/5 > 3/10)
    - Sáº¯p xáº¿p theo: Match ratio â†’ Sá»‘ nguyÃªn liá»‡u khá»›p â†’ Popularity
    """
    try:
        if not req.ingredients:
            return {"query": "No ingredients provided", "hits": []}
        
        print(f"[Search Ingredients] User ingredients: {req.ingredients}")

        # Chuáº©n hÃ³a user ingredients
        user_ings = set(ing.strip().lower() for ing in req.ingredients)
        
        # Láº¥y Táº¤T Cáº¢ recipes tá»« ChromaDB (khÃ´ng dÃ¹ng vector search)
        results = collection.get(include=["metadatas"])

        hits = []
        if results and results.get("ids"):
            for i, rid in enumerate(results["ids"]):
                meta = results["metadatas"][i]
                
                # Parse ingredients vÃ  tags tá»« recipe
                hit_ings = set(ing.strip().lower() for ing in meta.get("ingredients", "").split(", ") if ing.strip())
                hit_tags = set(tag.strip().lower() for tag in meta.get("tags", "").split(", ") if tag.strip())
                user_tags = set(tag.strip().lower() for tag in req.tags)

                # === FILTERS ===
                # Tags filter
                tags_match = user_tags.issubset(hit_tags) if user_tags else True
                if not tags_match:
                    continue

                # Cuisine filter
                hit_cuisine = meta.get("cuisineLowercase", "")
                user_cuisine = req.cuisine.lower() if req.cuisine else None
                cuisine_match = not user_cuisine or hit_cuisine == user_cuisine
                if not cuisine_match:
                    continue

                # Category filter
                hit_category = meta.get("categoryLowercase", "")
                user_category = req.category.lower() if req.category else None
                category_match = not user_category or hit_category == user_category
                if not category_match:
                    continue

                # === INGREDIENT MATCHING ===
                matched_ings = user_ings.intersection(hit_ings)
                matched_count = len(matched_ings)
                total_ings = len(hit_ings)
                
                # Tá»ˆ Lá»† MATCH: Sá»‘ nguyÃªn liá»‡u user cÃ³ / Tá»•ng nguyÃªn liá»‡u cá»§a mÃ³n
                match_ratio = matched_count / total_ings if total_ings > 0 else 0
                
                # USER COVERAGE: Bao nhiÃªu % nguyÃªn liá»‡u user Ä‘Æ°á»£c sá»­ dá»¥ng
                user_coverage = matched_count / len(user_ings) if user_ings else 0
                
                # === NGÆ¯á» NG Lá»ŒC ===
                # Rule 1: MÃ³n PHáº¢I match Ã­t nháº¥t 60% tá»•ng nguyÃªn liá»‡u
                MIN_MATCH_RATIO = 0.6  # 60%
                if match_ratio < MIN_MATCH_RATIO:
                    print(f"[Search] âŒ Skip '{meta.get('name')}' - Match {matched_count}/{total_ings} = {match_ratio*100:.0f}% < {MIN_MATCH_RATIO*100:.0f}%")
                    continue
                
                # Rule 2: Hoáº·c mÃ³n match Ã­t nháº¥t 80% nguyÃªn liá»‡u user nháº­p (cho phÃ©p user nháº­p thÃªm 1-2 thá»©)
                MIN_USER_COVERAGE = 0.8  # 80%
                if user_coverage < MIN_USER_COVERAGE and match_ratio < 0.8:
                    print(f"[Search] âŒ Skip '{meta.get('name')}' - User coverage {matched_count}/{len(user_ings)} = {user_coverage*100:.0f}% < {MIN_USER_COVERAGE*100:.0f}%")
                    continue
                
                print(f"[Search] âœ… Match '{meta.get('name')}' - {matched_count}/{total_ings} ({match_ratio*100:.0f}%), user coverage: {user_coverage*100:.0f}%")

                # === SCORING ===
                # 1. Match ratio score (cÃ ng cao cÃ ng tá»‘t)
                ratio_score = match_ratio * 1000
                
                # 2. Matched count score (cÃ ng nhiá»u cÃ ng tá»‘t)
                count_score = matched_count * 50
                
                # 3. Recipe size penalty (Æ°u tiÃªn mÃ³n Ã­t nguyÃªn liá»‡u hÆ¡n)
                # MÃ³n 3/5 nguyÃªn liá»‡u > MÃ³n 3/10 nguyÃªn liá»‡u
                size_penalty = -total_ings * 5
                
                # 4. User coverage bonus (sá»­ dá»¥ng háº¿t nguyÃªn liá»‡u user cÃ³)
                coverage_bonus = user_coverage * 200
                
                # 5. Popularity score (rating tháº¥p Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng nhiá»u)
                rate = meta.get("rate", 0.0)
                num_rates = meta.get("numberOfRate", 0)
                popularity_score = (rate / 5.0) * math.log(1 + num_rates) * 30

                # === Tá»”NG Há»¢P ===
                relevance_score = (
                    ratio_score +           # Æ¯u tiÃªn cao nháº¥t: Tá»‰ lá»‡ match
                    count_score +           # Sá»‘ lÆ°á»£ng nguyÃªn liá»‡u khá»›p
                    size_penalty +          # Penalty cho mÃ³n nhiá»u nguyÃªn liá»‡u
                    coverage_bonus +        # Bonus cho sá»­ dá»¥ng háº¿t nguyÃªn liá»‡u user cÃ³
                    popularity_score        # Popularity (tháº¥p)
                )

                ingredients_raw = meta.get("ingredients", "")
                ingredients_list = [ing.strip() for ing in ingredients_raw.split(",") if ing.strip()]

                hits.append({
                    "id": meta.get("id"),
                    "name": meta.get("name"),
                    "short": meta.get("short", ""),
                    "image": meta.get("image", ""),
                    "calories": meta.get("calories", 0),
                    "time": meta.get("time", ""),
                    "size": meta.get("size", ""),
                    "difficulty": meta.get("difficulty", ""),
                    "cuisine": meta.get("cuisine", ""),
                    "category": meta.get("category", ""),
                    "rate": rate,
                    "numberOfRate": num_rates,
                    "ingredients": ingredients_list,
                    "relevance_score": relevance_score,
                    "match_ratio": match_ratio,
                    "matched_count": matched_count,
                    "total_ingredients": total_ings,
                    "user_coverage": user_coverage
                })

        # Sort theo: 1. Match ratio, 2. Matched count, 3. Popularity
        hits = sorted(hits, key=lambda x: (x["match_ratio"], x["matched_count"], x["rate"]), reverse=True)[:req.top_k]
        
        print(f"[Search Ingredients] Found {len(hits)} results with match_ratio â‰¥ {0.6*100:.0f}%")

        return {"query": f"Ingredients: {', '.join(req.ingredients)}", "hits": hits}
    except Exception as e:
        print(f"[Search] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/search/search-by-keyword")
def search_by_keyword(req: KeywordSearchRequest):
    """
    ðŸ” HYBRID SEARCH - Káº¿t há»£p Text Matching + Vector Search:
    
    1. **Vector Search (Semantic)**: TÃ¬m mÃ³n Äƒn cÃ³ nghÄ©a tÆ°Æ¡ng tá»±
       - "gÃ  chiÃªn" â†’ "gÃ  rÃ¡n", "gÃ  giÃ²n", "gÃ  KFC"
       - "phá»Ÿ bÃ²" â†’ "phá»Ÿ tÃ¡i", "phá»Ÿ náº¡m", "bÃºn bÃ²"
       - Trá»ng sá»‘ cao (3.0) Ä‘á»ƒ khai thÃ¡c sá»©c máº¡nh AI
    
    2. **Text Matching (Lexical)**: Äáº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c
       - Exact match: Khá»›p 100% (Ä‘iá»ƒm cao)
       - Phrase match: Chuá»—i tá»« liÃªn tiáº¿p
       - Word match: Tá»«ng tá»« riÃªng láº»
       - Trá»ng sá»‘ vá»«a pháº£i (2.0)
    
    3. **Filtering**: Tags, Cuisine, Category
    
    â†’ Káº¿t quáº£: Vá»«a chÃ­nh xÃ¡c (text) vá»«a thÃ´ng minh (vector)
    """
    try:
        # Chuáº©n hÃ³a query
        keywords = req.keywords.strip()
        if not keywords:
            return {"query": "", "hits": []}
        
        keywords_lower = keywords.lower()
        keywords_no_accent = unidecode(keywords_lower)
        keyword_list = [kw.strip() for kw in keywords_no_accent.split() if kw.strip()]
        
        if not keyword_list:
            return {"query": keywords, "hits": []}

        print(f"[Search Keyword] Query: '{keywords}' â†’ Normalized: {keyword_list}")

        # Heuristic: action words (e.g., 'xÃ o') should appear in result names
        # Map query action tokens to allowed synonyms in names
        action_map = {
            "xao": {"xao", "chien", "ran"},
            "xÃ o": {"xao", "chien", "ran"},
            "chien": {"chien", "ran"},
            "rÃ¡n": {"ran", "chien"},
            "ran": {"ran", "chien"}
        }
        query_actions = {tok for tok in keyword_list if tok in action_map}

        # Build query text vá»›i trá»ng sá»‘ cao cho tÃªn mÃ³n
        q = f"{keywords}. MÃ³n Äƒn: {keywords}. TÃ¬m kiáº¿m: {keywords}"

        # Encode query thÃ nh vector
        q_emb = embed_model.encode(q).tolist()

        # Query ChromaDB vá»›i nhiá»u káº¿t quáº£ hÆ¡n Ä‘á»ƒ cÃ³ thá»ƒ re-rank
        results = collection.query(
            query_embeddings=[q_emb],
            n_results=min(req.top_k * 5, 100),  # Láº¥y nhiá»u Ä‘á»ƒ re-rank
            include=["metadatas", "distances"]
        )

        hits = []
        MIN_VECTOR_SCORE = 420   # Háº¡ ngÆ°á»¡ng Ä‘á»ƒ giá»¯ thÃªm mÃ³n liÃªn quan (phá»Ÿ gÃ /tÃ¡i/xÃ o, bÃ² xÃ o,...)
        RELAX_VECTOR_SCORE = 300 # NgÆ°á»¡ng ná»›i lá»ng náº¿u káº¿t quáº£ quÃ¡ Ã­t
        MIN_RESULTS_SOFT = max(5, req.top_k // 2)  # Náº¿u Ã­t hÆ¡n má»©c nÃ y sáº½ ná»›i lá»ng
        if results and results.get("ids"):
            for i, rid in enumerate(results["ids"][0]):
                meta = results["metadatas"][0][i]
                
                # Láº¥y cÃ¡c text fields Ä‘á»ƒ matching
                name = meta.get('name', '')
                name_lower = meta.get('nameLowercase', name.lower())
                name_no_accent = meta.get('nameNoAccent', unidecode(name_lower))
                short = meta.get('short', '').lower()
                short_no_accent = unidecode(short)
                
                # Kiá»ƒm tra filters trÆ°á»›c (nhanh hÆ¡n)
                # Tags filter
                hit_tags = set(tag.strip().lower() for tag in meta.get("tags", "").split(", ") if tag.strip())
                user_tags = set(tag.strip().lower() for tag in req.tags)
                tags_match = user_tags.issubset(hit_tags) if user_tags else True
                
                if not tags_match:
                    continue

                # Cuisine filter
                hit_cuisine = meta.get("cuisineLowercase", "")
                user_cuisine = req.cuisine.lower() if req.cuisine else None
                cuisine_match = not user_cuisine or hit_cuisine == user_cuisine
                
                if not cuisine_match:
                    continue

                # Category filter
                hit_category = meta.get("categoryLowercase", "")
                user_category = req.category.lower() if req.category else None
                category_match = not user_category or hit_category == user_category
                
                if not category_match:
                    continue

                # === VECTOR SEARCH - SEMANTIC SIMILARITY ===
                # ÄÃ¢y lÃ  pháº§n QUAN TRá»ŒNG NHáº¤T - AI hiá»ƒu nghÄ©a cá»§a query
                distance = results["distances"][0][i]
                # Chuyá»ƒn distance â†’ similarity score (distance cÃ ng nhá» = cÃ ng giá»‘ng)
                vector_score = 1000 / (1 + distance * 2) if distance >= 0 else 0
                
                print(f"[Vector] '{meta.get('name')}' - distance={distance:.3f}, vector_score={vector_score:.1f}")

                # === TEXT MATCHING - LEXICAL PRECISION ===
                # Äáº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c báº±ng text matching
                name = meta.get('name', '')
                name_lower = meta.get('nameLowercase', name.lower())
                name_no_accent = meta.get('nameNoAccent', unidecode(name_lower))
                short = meta.get('short', '').lower()
                short_no_accent = unidecode(short)
                
                # 1. EXACT MATCH - Khá»›p 100%
                exact_match_score = 0
                if keywords_no_accent == name_no_accent:
                    exact_match_score = 1000
                    print(f"[Text] âœ… EXACT MATCH: '{name}'")
                elif keywords_lower == name_lower:
                    exact_match_score = 900
                
                # 2. PHRASE MATCH - Chuá»—i tá»« xuáº¥t hiá»‡n liÃªn tiáº¿p
                phrase_match_score = 0
                if len(keyword_list) >= 2:
                    query_phrase = " ".join(keyword_list)
                    if query_phrase in name_no_accent:
                        phrase_match_score = 500
                        print(f"[Text] âœ… PHRASE MATCH in name: '{name}'")
                    elif query_phrase in short_no_accent:
                        phrase_match_score = 200
                
                # 3. WORD MATCH - Tá»«ng tá»« riÃªng láº»
                name_words = set(name_no_accent.split())
                matched_in_name = sum(1 for kw in keyword_list if kw in name_words)
                
                word_match_score = 0
                if matched_in_name > 0:
                    match_percentage = matched_in_name / len(keyword_list)
                    word_match_score = match_percentage * 300
                    print(f"[Text] Words matched: {matched_in_name}/{len(keyword_list)} in '{name}'")
                
                # TEXT MATCHING TOTAL
                text_match_score = exact_match_score + phrase_match_score + word_match_score
                
                # === POPULARITY SCORE ===
                rate = meta.get("rate", 0.0)
                num_rates = meta.get("numberOfRate", 0)
                popularity_score = (rate / 5.0) * math.log(1 + num_rates) * 50

                # === ACTION WORD HEURISTIC ===
                # If query contains an action word (e.g., 'xao'), ensure name includes a synonym
                action_required = len(query_actions) > 0
                action_ok = True
                if action_required:
                    action_ok = False
                    for act in query_actions:
                        synonyms = action_map.get(act, set())
                        if any(syn in name_no_accent for syn in synonyms) or any(syn in short_no_accent for syn in synonyms):
                            action_ok = True
                            break

                # === Tá»”NG Há»¢P ÄIá»‚M (HYBRID APPROACH) ===
                # ðŸŽ¯ CÃ´ng thá»©c cÃ¢n báº±ng giá»¯a Text vÃ  Vector
                # 
                # Vector (3.0): TÃ¬m mÃ³n tÆ°Æ¡ng tá»± vá» nghÄ©a - Cao nháº¥t Ä‘á»ƒ khai thÃ¡c AI
                # Text (2.0): Äáº£m báº£o chÃ­nh xÃ¡c - Exact match Ä‘Æ°á»£c Æ°u tiÃªn
                # Popularity (0.5): Boost nháº¹ - MÃ³n ngon, nhiá»u Ä‘Ã¡nh giÃ¡
                
                relevance_score = (
                    text_match_score * 2.0 +       # Text matching
                    vector_score * 3.0 +           # ðŸ”¥ VECTOR SEARCH - Trá»ng sá»‘ cao nháº¥t
                    popularity_score * 0.5         # Popularity boost
                )
                
                # === RELEVANCE FILTERS ===
                # YÃªu cáº§u: text Ä‘á»§ máº¡nh HOáº¶C vector >= MIN_VECTOR_SCORE
                short_query = len(keyword_list) <= 2
                name_match_pct = (matched_in_name / len(keyword_list)) if len(keyword_list) > 0 else 0.0
                required_pct = 0.7 if short_query else 0.6  # ná»›i lá»ng cho truy váº¥n ngáº¯n Ä‘á»ƒ ra thÃªm mÃ³n liÃªn quan
                strong_text = (
                    exact_match_score > 0 or
                    phrase_match_score > 0 or
                    name_match_pct >= required_pct
                )

                if relevance_score <= 0:
                    continue
                if not strong_text and vector_score < MIN_VECTOR_SCORE:
                    # Too weak semantically and lexically
                    continue
                if action_required and not action_ok:
                    # Query asked for an action (e.g., 'xÃ o') but name doesn't reflect it
                    continue
                
                print(f"[Score] '{meta.get('name')}' â†’ Total={relevance_score:.1f} (text={text_match_score:.0f}, vector={vector_score:.1f})")

                # Build response
                ingredients_raw = meta.get("ingredients", "")
                ingredients_list = [ing.strip() for ing in ingredients_raw.split(",") if ing.strip()]
                
                hits.append({
                    "id": meta.get("id"),
                    "name": meta.get("name"),
                    "short": meta.get("short", ""),
                    "image": meta.get("image", ""),
                    "calories": meta.get("calories", 0),
                    "time": meta.get("time", ""),
                    "size": meta.get("size", ""),
                    "difficulty": meta.get("difficulty", ""),
                    "cuisine": meta.get("cuisine", ""),
                    "category": meta.get("category", ""),
                    "rate": rate,
                    "numberOfRate": num_rates,
                    "ingredients": ingredients_list,
                    "distance": distance,
                    "relevance_score": relevance_score,
                    "vector_score": vector_score,
                    "text_score": text_match_score
                })

        # Náº¿u káº¿t quáº£ quÃ¡ Ã­t, ná»›i lá»ng nháº¹ (giá»¯ mÃ³n vector >= RELAX_VECTOR_SCORE hoáº·c name_match_pct >= 0.5)
        if len(hits) < MIN_RESULTS_SOFT and results and results.get("ids"):
            relaxed = []
            for i, rid in enumerate(results["ids"][0]):
                meta = results["metadatas"][0][i]
                distance = results["distances"][0][i]
                vector_score = 1000 / (1 + distance * 2) if distance >= 0 else 0
                name_no_accent = meta.get('nameNoAccent', unidecode(meta.get('name', '').lower()))
                name_words = set(name_no_accent.split())
                matched_in_name = sum(1 for kw in keyword_list if kw in name_words)
                name_match_pct = (matched_in_name / len(keyword_list)) if len(keyword_list) > 0 else 0.0
                if action_required:
                    short_no_accent = unidecode(meta.get('short', '').lower())
                    action_ok = any(any(syn in txt for syn in action_map.get(act, set())) for txt in [name_no_accent, short_no_accent] for act in query_actions)
                    if not action_ok:
                        continue
                if vector_score >= RELAX_VECTOR_SCORE or name_match_pct >= 0.5:
                    # Recompute scores quickly
                    exact_match_score = 1000 if keywords_no_accent == name_no_accent else 0
                    phrase_match_score = 0
                    if len(keyword_list) >= 2:
                        qp = " ".join(keyword_list)
                        if qp in name_no_accent:
                            phrase_match_score = 500
                    word_match_score = name_match_pct * 300
                    text_match_score = exact_match_score + phrase_match_score + word_match_score
                    relevance_score = text_match_score * 2.0 + vector_score * 3.0
                    ingredients_raw = meta.get("ingredients", "")
                    ingredients_list = [ing.strip() for ing in ingredients_raw.split(",") if ing.strip()]
                    relaxed.append({
                        "id": meta.get("id"),
                        "name": meta.get("name"),
                        "short": meta.get("short", ""),
                        "image": meta.get("image", ""),
                        "calories": meta.get("calories", 0),
                        "time": meta.get("time", ""),
                        "size": meta.get("size", ""),
                        "difficulty": meta.get("difficulty", ""),
                        "cuisine": meta.get("cuisine", ""),
                        "category": meta.get("category", ""),
                        "rate": meta.get("rate", 0.0),
                        "numberOfRate": meta.get("numberOfRate", 0),
                        "ingredients": ingredients_list,
                        "distance": distance,
                        "relevance_score": relevance_score,
                        "vector_score": vector_score,
                        "text_score": text_match_score
                    })
            hits = (hits + relaxed)

        # Sort theo relevance_score giáº£m dáº§n vÃ  Ã¡p dá»¥ng top_k
        hits = sorted(hits, key=lambda x: x["relevance_score"], reverse=True)[:req.top_k]
        
        print(f"[Search Keyword] Found {len(hits)} results")
        if hits:
            print(f"[Search Keyword] Top result: '{hits[0]['name']}' (total={hits[0]['relevance_score']:.1f}, vector={hits[0]['vector_score']:.1f}, text={hits[0]['text_score']:.1f})")

        return {"query": keywords, "hits": hits}
    except Exception as e:
        print(f"[Search] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@router.post("/reindex")
async def reindex_data():
    sync_recipes_to_chroma()
    return {"message": "Reindex completed"}